Context of transmission
Information content of outcomes
1/2
1/6
Information of A \cup B, A \cap B
https://en.wikipedia.org/wiki/Entropy_(information_theory)#Rationale and interpretations
Principle of maximum entropy
Giffin and Caticha (2007) state that Bayes' theorem and the principle of maximum entropy are completely compatible and can be seen as special cases of the "method of maximum relative entropy". They state that this method reproduces every aspect of orthodox Bayesian inference methods. In addition this new method opens the door to tackling problems that could not be addressed by either the maximal entropy principle or orthodox Bayesian methods individually. Moreover, recent contributions (Lazar 2003, and Schennach 2005) show that frequentist relative-entropy-based inference approaches (such as empirical likelihood and exponentially tilted empirical likelihood – see e.g. Owen 2001 and Kitamura 2006) can be combined with prior information to perform Bayesian posterior analysis.
Depends on the receivers model!!
Of course, in reality there is only one real state of the system. The entropy is not a direct function of that state. It is a function of the real state only through the (subjectively chosen) macroscopic model description.
Inefficient English. Language models
1/4 * 2 bits + 3/4 * 1 bit
Entropy of random variable
comedian George Carlin, “Weather forecast for tonight: dark. Continued dark overnight, with widely scattered light by morning.” Assuming one not residing near the Earth's poles or polar circles, the amount of information conveyed in that forecast is zero because it is known, in advance of receiving the forecast, that darkness always comes with the night.
Mutual information
Conditional entropy
Kolmogorov complexity
KL-divergence the expectation of the logarithmic difference between the probabilities P and Q, where the expectation is taken using the probabilities P
Fischer information is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter θ of a distribution that models X.
Information in common distributions
Hartly (hart), nat, bit, shannon (sh), ban, nit, nepit, nibble, byte
Statistical thermo
"In information theoretic terms, the information entropy of a system is the amount of "missing" information needed to determine a microstate, given the macrostate.”
Landauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation.
Maxwell’s demon.
Minimum message length (MML) is a formal information theory restatement of Occam's Razor
Compression algorithms
Shannon's theorem also implies that no lossless compression scheme can shorten all messages. If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger. However, the problem can still arise even in everyday use when applying a compression algorithm to already compressed data: for example, making a ZIP file of music, pictures or videos that are already in a compressed format such as FLAC, MP3, WebM, AAC, PNG or JPEG will generally result in a ZIP file that is slightly larger than the source file(s).
Hufmann encoding
Information criteria

https://link.springer.com/article/10.1007/BF00134086
https://en.wikipedia.org/wiki/Maximum_entropy_thermodynamics
